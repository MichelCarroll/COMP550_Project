{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-18T20:57:12.839638996Z",
     "start_time": "2023-12-18T20:57:10.597035122Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from src.dataset_utils import MotleyFoolDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /export/livia/home/vision/Ddjomby/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /export/livia/home/vision/Ddjomby/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /export/livia/home/vision/Ddjomby/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /export/livia/home/vision/Ddjomby/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger') "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T20:57:13.101748074Z",
     "start_time": "2023-12-18T20:57:12.841518396Z"
    }
   },
   "id": "2e1da723da2812df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load and prepare dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "918d7ecf3b17355"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Should be `(1141, 3821, 33174)`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc8cf49d3c98dc86"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(1141, 3821, 33174)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MotleyFoolDataset('TMF_dataset_annotated.zip')\n",
    "test_set = dataset['Q1 2023':'Q4 2023']\n",
    "val_set = dataset['Q1 2022':'Q4 2022']\n",
    "train_set = dataset[:'Q4 2021']\n",
    "\n",
    "len(test_set), len(val_set), len(train_set)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T20:57:14.314659015Z",
     "start_time": "2023-12-18T20:57:13.102964342Z"
    }
   },
   "id": "5ea45bc66d7de0d7"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "X_train_val = [instance['content'] for instance in dataset[:'Q4 2022']]\n",
    "y_train_val = [instance['direction'] == 'UP' for instance in dataset[:'Q4 2022']]\n",
    "\n",
    "X_test = [instance['content'] for instance in test_set]\n",
    "y_test = [instance['direction'] == 'UP' for instance in test_set]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T20:57:31.982095110Z",
     "start_time": "2023-12-18T20:57:14.316292800Z"
    }
   },
   "id": "7dda7b153a26386c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utilities for the grid search"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dba5e78714cdc46"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords.words('english'))  # set for membership check optim\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "\n",
    "### Adapted from: https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "treebank_mapping = {\n",
    "    'J': wordnet.ADJ,\n",
    "    'V': wordnet.VERB,\n",
    "    'N': wordnet.NOUN,\n",
    "    'R': wordnet.ADV\n",
    "}\n",
    "treebank_default = wordnet.NOUN\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    return treebank_mapping.get(treebank_tag[0], treebank_default)\n",
    "\n",
    "        \n",
    "# Define custom NLTK tokenizers\n",
    "def porter_tokenizer(text):\n",
    "    return [porter.stem(word) for word in word_tokenize(text) if word not in stopwords_set]\n",
    "\n",
    "def snowball_tokenizer(text):\n",
    "    return [snowball.stem(word) for word in word_tokenize(text) if word not in stopwords_set]\n",
    "\n",
    "def wordnet_lemmatizer_tokenizer(text):\n",
    "    text = word_tokenize(text)\n",
    "    tags = nltk.pos_tag(text)\n",
    "    return [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
    "        for word, tag in tags\n",
    "        if word not in stopwords_set\n",
    "    ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T20:57:34.058435502Z",
     "start_time": "2023-12-18T20:57:31.972669548Z"
    }
   },
   "id": "b616e7bf26de59cf"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "vect_parameter_grid = {\n",
    "    \"vect__tokenizer\": (\n",
    "        wordnet_lemmatizer_tokenizer,\n",
    "        porter_tokenizer,\n",
    "        snowball_tokenizer,\n",
    "    ),\n",
    "    \"vect__max_df\": (0.2, 0.5, 0.8, 1.0),\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2), (2, 2)),  # unigrams or bigrams\n",
    "    \"vect__norm\": (\"l1\", \"l2\", None),\n",
    "}\n",
    "\n",
    "vectorizer_factory = lambda: (\"vect\", TfidfVectorizer(\n",
    "    stop_words=None,\n",
    "    token_pattern=None, # will not be used since 'tokenizer' is not None\n",
    "))\n",
    "\n",
    "def do_grid_search(parameter_grid, classifier=None, pipeline=None):\n",
    "  parameter_grid |= vect_parameter_grid\n",
    "\n",
    "  if pipeline is None:\n",
    "    pipeline = Pipeline([\n",
    "      vectorizer_factory(),\n",
    "      (\"clf\", classifier),\n",
    "    ])\n",
    "\n",
    "  grid_search = GridSearchCV(\n",
    "      estimator=pipeline,\n",
    "      scoring='accuracy',\n",
    "      param_grid=parameter_grid,\n",
    "      n_jobs=-1,\n",
    "      cv=[  # Remove cross validation and use our pre-made split\n",
    "          # An iterable yielding (train, test) splits as arrays of indices.\n",
    "          (np.arange(*dataset.range(q_stop='Q4 2021')),\n",
    "           np.arange(*dataset.range('Q1 2022', 'Q4 2022')))\n",
    "      ],\n",
    "      verbose=3,\n",
    "  )\n",
    "\n",
    "  grid_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "  print('Best parameters:')\n",
    "  best_parameters = grid_search.best_estimator_.get_params()\n",
    "  for param_name in sorted(parameter_grid.keys()):\n",
    "      print(f\"{param_name}: {best_parameters[param_name]}\")\n",
    "\n",
    "\n",
    "  y_pred = grid_search.predict(X_test)\n",
    "  print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "  print('Accuracy score:', accuracy_score(y_test, y_pred))\n",
    "  print(classification_report(y_test, y_pred)) \n",
    "\n",
    "  return grid_search"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T20:57:34.078420829Z",
     "start_time": "2023-12-18T20:57:34.064297842Z"
    }
   },
   "id": "1ad48e83a26c227"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40c0b202a17b3afe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35928d6ccb0d649"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1404 candidates, totalling 1404 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"/export/livia/home/vision/Ddjomby/anaconda3/envs/comp550/lib/python3.11/site-packages/joblib/__init__.py\", line 129, in <module>\n",
      "    from .parallel import Parallel\n",
      "  File \"/export/livia/home/vision/Ddjomby/anaconda3/envs/comp550/lib/python3.11/site-packages/joblib/parallel.py\", line 31, in <module>\n",
      "    from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,\n",
      "  File \"/export/livia/home/vision/Ddjomby/anaconda3/envs/comp550/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 16, in <module>\n",
      "    from .pool import MemmappingPool\n",
      "  File \"/export/livia/home/vision/Ddjomby/anaconda3/envs/comp550/lib/python3.11/site-packages/joblib/pool.py\", line 31, in <module>\n",
      "    from ._memmapping_reducer import get_memmapping_reducers\n",
      "  File \"/export/livia/home/vision/Ddjomby/anaconda3/envs/comp550/lib/python3.11/site-packages/joblib/_memmapping_reducer.py\", line 37, in <module>\n",
      "    from .externals.loky.backend import resource_tracker\n",
      "  File \"/export/livia/home/vision/Ddjomby/anaconda3/envs/comp550/lib/python3.11/site-packages/joblib/externals/loky/__init__.py\", line 20, in <module>\n",
      "    from .reusable_executor import get_reusable_executor\n",
      "  File \"/export/livia/home/vision/Ddjomby/anaconda3/envs/comp550/lib/python3.11/site-packages/joblib/externals/loky/reusable_executor.py\", line 11, in <module>\n",
      "    from .process_executor import ProcessPoolExecutor, EXTRA_QUEUED_CALLS\n",
      "  File \"/export/livia/home/vision/Ddjomby/anaconda3/envs/comp550/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 83, in <module>\n",
      "    from .backend.queues import Queue, SimpleQueue\n",
      "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 936, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1069, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 729, in _compile_bytecode\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "nb_gs = do_grid_search(\n",
    "    {\"clf__alpha\": np.logspace(-6, 6, 13)},\n",
    "    MultinomialNB()\n",
    ")\n",
    "\n",
    "dump(nb_gs.best_estimator_, 'nb_gs.joblib') "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-18T20:57:34.080559389Z"
    }
   },
   "id": "24a61ec032ba6db4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf250732551374f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lf_gs = do_grid_search(\n",
    "    {\n",
    "        \"clf__penalty\": ('l1', 'l2', 'elasticnet', None),\n",
    "        \"clf__C\": np.logspace(-6, 6, 13),\n",
    "    },\n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "dump(nb_gs.best_estimator_, 'nb_gs.joblib') "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9b5f6e1280b75a73"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Support Vector Machine"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5f894fa446ea7a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear SVC"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e456b19886ac25a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svc_gs = do_grid_search(\n",
    "    {\"clf__C\": np.logspace(-6, 6, 13)},\n",
    "    pipeline=Pipeline([\n",
    "      vectorizer_factory(),\n",
    "      (\"lsa\", TruncatedSVD(n_components=100)),\n",
    "      (\"norm\", StandardScaler()),\n",
    "      (\"clf\", SVC(kernel='linear')) # So that it still is a linear classifier\n",
    "    ])\n",
    ")\n",
    "\n",
    "dump(nb_gs.best_estimator_, 'nb_gs.joblib') "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7657e4cf1d2fa95d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Non-linear SVC"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42c6e2f1cdb1b4e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "HistGradientBoostingClassifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b881589e34a4c2fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gradient Boosted Tree"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bffcdeb44956f776"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Friedman, J.H. (2002). Stochastic gradient boosting. Computational Statistics & Data Analysis, 38, 367-378.\n",
    "gbdt_gs = do_grid_search(\n",
    "    {\n",
    "        \"clf__learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "    },\n",
    "    pipeline=Pipeline([\n",
    "      vectorizer_factory(),\n",
    "      (\"clf\", HistGradientBoostingClassifier())\n",
    "    ])\n",
    ")\n",
    "\n",
    "dump(gbdt_gs.best_estimator_, 'gbdt_gs.joblib') "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a12029adcfe515f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
