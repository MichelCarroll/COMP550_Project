{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-16T20:30:28.622503900Z",
     "start_time": "2023-12-16T20:30:26.444212Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from src.dataset_utils import MotleyFoolDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\damie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\damie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\damie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\damie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger') "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T20:30:28.934905100Z",
     "start_time": "2023-12-16T20:30:28.623610600Z"
    }
   },
   "id": "2e1da723da2812df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load and prepare dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "918d7ecf3b17355"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Should be `(1141, 3821, 33174)`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc8cf49d3c98dc86"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(1141, 3821, 33174)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MotleyFoolDataset('TMF_dataset_annotated.zip')\n",
    "test_set = dataset['Q1 2023':'Q4 2023']\n",
    "val_set = dataset['Q1 2022':'Q4 2022']\n",
    "train_set = dataset[:'Q4 2021']\n",
    "\n",
    "len(test_set), len(val_set), len(train_set)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T20:30:30.305967800Z",
     "start_time": "2023-12-16T20:30:28.929140900Z"
    }
   },
   "id": "5ea45bc66d7de0d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_val = [instance['content'] for instance in dataset[:'Q4 2022']]\n",
    "y_train_val = [instance['direction'] == 'UP' for instance in dataset[:'Q4 2022']]\n",
    "\n",
    "X_test = [instance['content'] for instance in test_set]\n",
    "y_test = [instance['direction'] == 'UP' for instance in test_set]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7dda7b153a26386c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utilities for the grid search"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dba5e78714cdc46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### From: https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "stopwords_set = set(stopwords.words('english'))  # set for appartenance check optim\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def lemm_tokenizer(x):\n",
    "  x = word_tokenize(x)\n",
    "  tags = nltk.pos_tag(x)\n",
    "  return [\n",
    "      lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
    "      for word, tag in tags\n",
    "      if word not in stopwords_set\n",
    "  ]\n",
    "\n",
    "def porter_stem_tokenizer(x):\n",
    "  x = word_tokenize(x)\n",
    "  return [porter_stemmer.stem(word) for word in x if word not in stopwords_set]\n",
    "\n",
    "def snowball_stem_tokenizer(x):\n",
    "  x = word_tokenize(x)\n",
    "  return [snowball_stemmer.stem(word) for word in x if word not in stopwords_set]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b616e7bf26de59cf"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemm_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m vect_parameter_grid \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvect__tokenizer\u001B[39m\u001B[38;5;124m\"\u001B[39m: (\n\u001B[1;32m----> 3\u001B[0m         \u001B[43mlemm_tokenizer\u001B[49m,\n\u001B[0;32m      4\u001B[0m         porter_stem_tokenizer,\n\u001B[0;32m      5\u001B[0m         snowball_stem_tokenizer,\n\u001B[0;32m      6\u001B[0m     ),\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvect__max_df\u001B[39m\u001B[38;5;124m\"\u001B[39m: (\u001B[38;5;241m0.2\u001B[39m, \u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.8\u001B[39m, \u001B[38;5;241m1.0\u001B[39m),\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvect__ngram_range\u001B[39m\u001B[38;5;124m\"\u001B[39m: ((\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m), (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m), (\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m2\u001B[39m)),  \u001B[38;5;66;03m# unigrams or bigrams\u001B[39;00m\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvect__norm\u001B[39m\u001B[38;5;124m\"\u001B[39m: (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ml1\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ml2\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m     10\u001B[0m }\n\u001B[0;32m     12\u001B[0m vectorizer_factory \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m: (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvect\u001B[39m\u001B[38;5;124m\"\u001B[39m, TfidfVectorizer(\n\u001B[0;32m     13\u001B[0m     stop_words\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     14\u001B[0m     token_pattern\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;66;03m# will not be used since 'tokenizer' is not None\u001B[39;00m\n\u001B[0;32m     15\u001B[0m ))\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdo_grid_search\u001B[39m(parameter_grid, classifier\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, pipeline\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n",
      "\u001B[1;31mNameError\u001B[0m: name 'lemm_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "vect_parameter_grid = {\n",
    "    \"vect__tokenizer\": (\n",
    "        lemm_tokenizer,\n",
    "        porter_stem_tokenizer,\n",
    "        snowball_stem_tokenizer,\n",
    "    ),\n",
    "    \"vect__max_df\": (0.2, 0.5, 0.8, 1.0),\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2), (2, 2)),  # unigrams or bigrams\n",
    "    \"vect__norm\": (\"l1\", \"l2\", None),\n",
    "}\n",
    "\n",
    "vectorizer_factory = lambda: (\"vect\", TfidfVectorizer(\n",
    "    stop_words=None,\n",
    "    token_pattern=None, # will not be used since 'tokenizer' is not None\n",
    "))\n",
    "\n",
    "def do_grid_search(parameter_grid, classifier=None, pipeline=None):\n",
    "  parameter_grid |= vect_parameter_grid\n",
    "\n",
    "  if pipeline is None:\n",
    "    pipeline = Pipeline([\n",
    "      vectorizer_factory(),\n",
    "      (\"clf\", classifier),\n",
    "    ])\n",
    "\n",
    "  grid_search = GridSearchCV(\n",
    "      estimator=pipeline,\n",
    "      scoring='accuracy',\n",
    "      param_grid=parameter_grid,\n",
    "      #n_jobs=1, # Cannot use multiple processes due to tokenizer\n",
    "      cv=[  # Remove cross validation and use our pre-made split\n",
    "          # An iterable yielding (train, test) splits as arrays of indices.\n",
    "          (np.arange(*dataset.range(q_stop='Q4 2021')),\n",
    "           np.arange(*dataset.range('Q1 2022', 'Q4 2022')))\n",
    "      ]\n",
    "  )\n",
    "\n",
    "  grid_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "  print('Best parameters:')\n",
    "  best_parameters = grid_search.best_estimator_.get_params()\n",
    "  for param_name in sorted(parameter_grid.keys()):\n",
    "      print(f\"{param_name}: {best_parameters[param_name]}\")\n",
    "\n",
    "\n",
    "  y_pred = grid_search.predict(X_test)\n",
    "  ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "\n",
    "  print('Accuracy score:', accuracy_score(y_test, y_pred))\n",
    "  print(classification_report(y_test, y_pred)) \n",
    "\n",
    "  return grid_search"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T20:17:21.331052Z",
     "start_time": "2023-12-16T20:17:20.967497Z"
    }
   },
   "id": "1ad48e83a26c227"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40c0b202a17b3afe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35928d6ccb0d649"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "do_grid_search(\n",
    "    {\"clf__alpha\": np.logspace(-6, 6, 13)},\n",
    "    MultinomialNB()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24a61ec032ba6db4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf250732551374f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "do_grid_search(\n",
    "    {\n",
    "        \"clf__penalty\": ('l1', 'l2', 'elasticnet', None),\n",
    "        \"clf__C\": np.logspace(-6, 6, 13),\n",
    "    },\n",
    "    LogisticRegression()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b5f6e1280b75a73"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Support Vector Machine"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5f894fa446ea7a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear SVC"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e456b19886ac25a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "do_grid_search(\n",
    "    {\"clf__C\": np.logspace(-6, 6, 13)},\n",
    "    pipeline=Pipeline([\n",
    "      vectorizer_factory(),\n",
    "      (\"lsa\", TruncatedSVD(n_components=100)),\n",
    "      (\"norm\", Normalizer()),\n",
    "      (\"clf\", SVC(kernel='linear')) # So that it still is a linear classifier\n",
    "    ])\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a12029adcfe515f1"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "42c6e2f1cdb1b4e9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
